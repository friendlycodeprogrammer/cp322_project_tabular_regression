{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ce0a2d",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Datasets & Pre-Processing\n",
    "This notebook documents the data understanding + cleaning pipeline for the CP322 Topic 3 project. Execute each section in order to regenerate the cleaned splits stored under `data/processed/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b28c65",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dataset Overview\n",
    "We work with two open datasets so that modeling techniques can be compared across domains.\n",
    "\n",
    "- **House Prices (Kaggle)** – 1,460 residential properties from Ames, Iowa with 79 engineered features. Target is `SalePrice`. Mix of numeric (e.g., `GrLivArea`, `GarageArea`) and categorical (e.g., `Neighborhood`, `Exterior1st`) attributes with moderate missingness.\n",
    "- **Energy Efficiency (UCI)** – 768 building simulations describing wall/roof glazing characteristics. Targets are `Y1` (heating load) and `Y2` (cooling load). Inputs are all numeric but on different scales, so normalization is necessary.\n",
    "\n",
    "> Data dictionaries live in `data/raw/data_description.txt` (Kaggle) and within the UCI paper. Keep this cell updated if new fields are engineered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_theme()\n",
    "\n",
    "RAW_DIR = Path('data/raw')\n",
    "PROC_DIR = Path('data/processed')\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HOUSE_RAW_PATH = RAW_DIR / 'train.csv'\n",
    "HOUSE_TEST_PATH = RAW_DIR / 'test.csv'\n",
    "ENERGY_RAW_PATH = RAW_DIR / 'ENB2012_data.xlsx'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def describe_missing(df: pd.DataFrame, dataset_name: str, top_n: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"Return sorted missing-value summary for quick inspection.\"\"\"\n",
    "    missing = df.isna().mean().sort_values(ascending=False)\n",
    "    summary = missing[missing > 0].head(top_n).to_frame(name=f'{dataset_name}_pct_missing')\n",
    "    return summary\n",
    "\n",
    "print('House Prices rows:', pd.read_csv(HOUSE_RAW_PATH).shape)\n",
    "print('Energy Efficiency rows:', pd.read_excel(ENERGY_RAW_PATH).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13dc89",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Exploratory Data Analysis\n",
    "This section surfaces distributions and correlations that guide preprocessing decisions (log-transform targets, scale numeric columns, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- House Prices EDA -------------------------------------------------------\n",
    "house_df = pd.read_csv(HOUSE_RAW_PATH)\n",
    "\n",
    "house_target = 'SalePrice'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(house_df[house_target], kde=True, ax=axes[0])\n",
    "axes[0].set_title('SalePrice Distribution')\n",
    "axes[0].set_xlabel('SalePrice')\n",
    "\n",
    "sns.histplot(np.log1p(house_df[house_target]), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Log SalePrice Distribution')\n",
    "axes[1].set_xlabel('log1p(SalePrice)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr_cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'YearBuilt']\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(house_df[corr_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('House Prices – Top Feature Correlations')\n",
    "plt.show()\n",
    "\n",
    "describe_missing(house_df, 'house').head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Energy Efficiency EDA --------------------------------------------------\n",
    "energy_df = pd.read_excel(ENERGY_RAW_PATH)\n",
    "energy_df.columns = [c.strip() for c in energy_df.columns]\n",
    "energy_targets = ['Y1', 'Y2']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for idx, target in enumerate(energy_targets):\n",
    "    sns.histplot(energy_df[target], kde=True, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{target} Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(energy_df.corr(), cmap='viridis', annot=False)\n",
    "plt.title('Energy Efficiency – Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "describe_missing(energy_df, 'energy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076835d2",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Preprocessing Pipelines\n",
    "Steps applied to each dataset:\n",
    "\n",
    "1. **Split** into train/validation/test with a fixed random seed (70/15/15 split achieved via two splits).\n",
    "2. **Impute** missing numeric values with medians and categorical values with the most frequent category.\n",
    "3. **Encode** categoricals using `OneHotEncoder(handle_unknown='ignore')`.\n",
    "4. **Normalize** numeric columns using `StandardScaler`.\n",
    "5. **Persist** processed splits and minimal metadata for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper functions -------------------------------------------------------\n",
    "\n",
    "def split_dataframe(df: pd.DataFrame, target_cols, test_size=0.15, val_size=0.15):\n",
    "    y = df[target_cols]\n",
    "    X = df.drop(columns=target_cols)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE)\n",
    "    relative_val = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=relative_val, random_state=RANDOM_STATE)\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "\n",
    "def build_house_preprocessor(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if 'SalePrice' in num_cols:\n",
    "        num_cols.remove('SalePrice')\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_pipeline, num_cols),\n",
    "        ('cat', categorical_pipeline, cat_cols)\n",
    "    ])\n",
    "    return preprocessor, num_cols, cat_cols\n",
    "\n",
    "\n",
    "def build_energy_preprocessor(df: pd.DataFrame):\n",
    "    num_cols = df.columns.tolist()\n",
    "    for target in ['Y1', 'Y2']:\n",
    "        if target in num_cols:\n",
    "            num_cols.remove(target)\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_pipeline, num_cols)\n",
    "    ])\n",
    "    return preprocessor, num_cols\n",
    "\n",
    "\n",
    "def transform_to_frame(preprocessor, X, feature_prefix):\n",
    "    array = preprocessor.transform(X)\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    return pd.DataFrame(array, columns=[f'{feature_prefix}_{name}' for name in feature_names])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28350b4",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 House Prices Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(H_train, h_y_train), (H_val, h_y_val), (H_test, h_y_test) = split_dataframe(\n",
    "    house_df, target_cols=['SalePrice'])\n",
    "\n",
    "house_preprocessor, house_num_cols, house_cat_cols = build_house_preprocessor(house_df)\n",
    "house_preprocessor.fit(H_train)\n",
    "\n",
    "H_train_proc = transform_to_frame(house_preprocessor, H_train, 'house')\n",
    "H_val_proc = transform_to_frame(house_preprocessor, H_val, 'house')\n",
    "H_test_proc = transform_to_frame(house_preprocessor, H_test, 'house')\n",
    "\n",
    "for frame, target, split in [\n",
    "    (H_train_proc, h_y_train, 'train'),\n",
    "    (H_val_proc, h_y_val, 'val'),\n",
    "    (H_test_proc, h_y_test, 'test'),\n",
    "]:\n",
    "    dataset = frame.copy()\n",
    "    dataset['SalePrice'] = target.values\n",
    "    out_path = PROC_DIR / f'house_prices_{split}.csv'\n",
    "    dataset.to_csv(out_path, index=False)\n",
    "    print(f'Saved {out_path}')\n",
    "\n",
    "house_meta = {\n",
    "    'numeric_columns': house_num_cols,\n",
    "    'categorical_columns': house_cat_cols,\n",
    "    'feature_count': H_train_proc.shape[1],\n",
    "    'splits': {split: len(data) for split, data in {'train': H_train_proc, 'val': H_val_proc, 'test': H_test_proc}.items()}\n",
    "}\n",
    "\n",
    "with open(PROC_DIR / 'house_prices_metadata.json', 'w') as fh:\n",
    "    json.dump(house_meta, fh, indent=2)\n",
    "\n",
    "house_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231075c",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Energy Efficiency Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c851836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(En_train, e_y_train), (En_val, e_y_val), (En_test, e_y_test) = split_dataframe(\n",
    "    energy_df, target_cols=energy_targets)\n",
    "\n",
    "energy_preprocessor, energy_num_cols = build_energy_preprocessor(energy_df)\n",
    "energy_preprocessor.fit(En_train)\n",
    "\n",
    "En_train_proc = transform_to_frame(energy_preprocessor, En_train, 'energy')\n",
    "En_val_proc = transform_to_frame(energy_preprocessor, En_val, 'energy')\n",
    "En_test_proc = transform_to_frame(energy_preprocessor, En_test, 'energy')\n",
    "\n",
    "for frame, target, split in [\n",
    "    (En_train_proc, e_y_train, 'train'),\n",
    "    (En_val_proc, e_y_val, 'val'),\n",
    "    (En_test_proc, e_y_test, 'test'),\n",
    "]:\n",
    "    dataset = frame.copy()\n",
    "    dataset['Y1'] = target['Y1'].values\n",
    "    dataset['Y2'] = target['Y2'].values\n",
    "    out_path = PROC_DIR / f'energy_efficiency_{split}.csv'\n",
    "    dataset.to_csv(out_path, index=False)\n",
    "    print(f'Saved {out_path}')\n",
    "\n",
    "energy_meta = {\n",
    "    'numeric_columns': energy_num_cols,\n",
    "    'feature_count': En_train_proc.shape[1],\n",
    "    'splits': {split: len(data) for split, data in {'train': En_train_proc, 'val': En_val_proc, 'test': En_test_proc}.items()}\n",
    "}\n",
    "\n",
    "with open(PROC_DIR / 'energy_efficiency_metadata.json', 'w') as fh:\n",
    "    json.dump(energy_meta, fh, indent=2)\n",
    "\n",
    "energy_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41610c",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Validation\n",
    "Quick sanity checks to ensure no data leakage and that scaling produced zero-mean roughly unit-variance features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, df in {\n",
    "    'house_train': H_train_proc,\n",
    "    'energy_train': En_train_proc,\n",
    "}.items():\n",
    "    print(f\"\n",
    "{name} mean range: {df.mean().round(3).describe()['mean']} ± {df.mean().std():.3f}\")\n",
    "    print(f\"{name} std range: {df.std().round(3).describe()['mean']} ± {df.std().std():.3f}\")\n",
    "\n",
    "print('\n",
    "Re-run this notebook whenever raw data or feature engineering changes to refresh processed files.')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
