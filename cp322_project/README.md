# CP322 Machine Learning Project
### Regression on Tabular Data: House Prices & Energy Efficiency (Method 2 – Algorithmic Improvement)

**Team Members:** [List names & IDs]  
**Due Date:** Dec 3, 2025  

This repository contains all code, notebooks, processed datasets, and demo assets for Topic 3 – Method 2 (algorithmic improvement on tabular regression).

## Folder Structure
- `data/raw/` → Original datasets (House Prices, Energy Efficiency).
- `data/processed/` → Cleaned + split datasets regenerated by the preprocessing notebook.
- `notebooks/`
  - `data_preprocessing.ipynb` → dataset overview, EDA, preprocessing, and persistence.
  - `baselines.ipynb` → placeholder for baseline benchmarking (XGBoost, TabNet, etc.).
  - `improved_model.ipynb` → placeholder for methodological improvement experiments.
- `scripts/` → automation helpers (e.g., `train_baselines.py`).
- `models/` → Serialized pipelines (`house_price_model.pkl`, `energy_efficiency_model.pkl`) + metrics.
- `app/` → Streamlit demo that surfaces both prediction tasks.
- `docs/` → Demo screenshot guide and (future) report assets.

## Quick Start
```bash
git clone https://github.com/<yourusername>/cp322_project_tabular_regression.git
cd cp322_project_tabular_regression
python -m venv .venv && source .venv/bin/activate  # optional but recommended
pip install -r requirements.txt
```

## Data Preparation Workflow
1. Place raw sources inside `data/raw/`
   - Kaggle House Prices: `train.csv`, `test.csv`, etc.
   - UCI Energy Efficiency: `ENB2012_data.xlsx`.
2. Run the preprocessing notebook (or execute all cells via `jupyter nbconvert --execute notebooks/data_preprocessing.ipynb`) to:
   - Describe both datasets.
   - Handle missing values (median / most-frequent imputation), categorical encoding, and numeric normalization.
   - Produce EDA artifacts (histograms, correlation heatmaps, target distributions).
   - Split into train/validation/test (70/15/15) and save cleaned CSVs + metadata JSON under `data/processed/`.

Processed files are versioned so collaborators can reproduce modeling experiments without rerunning notebooks immediately.

## Baseline Models
Train the lightweight demo-ready regressors:
```bash
python scripts/train_baselines.py
```
This command fits:
- A RandomForest pipeline on a curated subset of House Prices features.
- A GradientBoosting-based multi-output regressor for Energy Efficiency.

Metrics are logged to `models/training_report.json`, and the fitted pipelines land in `models/`.

## Demo (Streamlit)
Launch the combined UI:
```bash
streamlit run app/app.py
```
- **House Prices tab:** sliders/selectors for key Ames attributes → predicted sale price.
- **Energy Usage tab:** building parameters (`X1`-`X8`) → heating/cooling load.
- Sample values + expected outputs for screenshots live in `docs/demo_samples.md`.

> Capture two screenshots (one per tab) using the provided sample inputs for inclusion in the report/presentation.

## Next Steps
- Fill in `notebooks/baselines.ipynb` with XGBoost + neural tabular comparisons.
- Use `notebooks/improved_model.ipynb` for methodological improvements (Optuna tuning, SHAP analysis, etc.).
- Wire advanced models into `app/app.py` as they become available (swap serialized pipeline paths).
